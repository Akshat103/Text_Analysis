{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/drax/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting article from URL ID: blackassign0001\n",
      "Extracting article from URL ID: blackassign0002\n",
      "Extracting article from URL ID: blackassign0003\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "def extract_article(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find('h1', class_='entry-title')\n",
    "    if title is None:\n",
    "        title = soup.find('h1', class_='tdb-title-text')\n",
    "    \n",
    "    title_text = title.get_text(strip=True) if title else None\n",
    "\n",
    "    article_content = soup.find('div', class_='td-post-content')\n",
    "    if article_content is None:\n",
    "        article_content = soup.find('div', class_='tdb-block-inner')\n",
    "    \n",
    "    article_text = article_content.get_text(strip=True) if article_content else None\n",
    "\n",
    "    return title_text, article_text\n",
    "\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "output_dir = 'extracted_articles'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    print(f'Extracting article from URL ID: {url_id}')\n",
    "    \n",
    "    title, article_text = extract_article(url)\n",
    "    if title and article_text:\n",
    "        file_path = os.path.join(output_dir, f'{url_id}.txt')\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(title + '\\n' + article_text)\n",
    "    else:\n",
    "        print(f'Failed to extract article from URL ID: {url_id}')\n",
    "\n",
    "print('Extraction complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words HashSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(path, encoding='latin1'):\n",
    "    stop_words = set()\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, file), 'r', encoding=encoding) as f:\n",
    "                words = f.read().split()\n",
    "                stop_words.update(words)\n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary(file_path, encoding='latin1'):\n",
    "    dictionary = set()\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        words = f.read().split()\n",
    "        dictionary.update(words)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters, all in lower case and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = {'sample', 'longer'}\n",
    "negative_words = {'complex'}\n",
    "\n",
    "sample_text = \"This is a sample sentence. This is another sample sentence, which is longer and more complex.\"\n",
    "sample_tokens = word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(tokens):\n",
    "    return sum(1 for word in tokens if word in positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "positive_score = calculate_positive_score(sample_tokens)\n",
    "print(positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_negative_score(tokens):\n",
    "    return sum(1 for word in tokens if word in negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "negative_score = calculate_negative_score(sample_tokens)\n",
    "print(negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49999987500003124\n"
     ]
    }
   ],
   "source": [
    "polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "print(polarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "    return (positive_score + negative_score) / (total_words + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21052630470914185\n"
     ]
    }
   ],
   "source": [
    "total_words = len(sample_tokens)\n",
    "subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)\n",
    "print(subjectivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words) / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5\n"
     ]
    }
   ],
   "source": [
    "avg_sentence_length = calculate_avg_sentence_length(sample_text)\n",
    "print(avg_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_complex_words(tokens):\n",
    "    complex_words = [word for word in tokens if sum(1 for char in word if char in 'aeiou') > 2]\n",
    "    return len(complex_words) / len(tokens) if tokens else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15789473684210525\n"
     ]
    }
   ],
   "source": [
    "percentage_complex_words = calculate_percentage_complex_words(sample_tokens)\n",
    "print(percentage_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fog_index(avg_sentence_length, percentage_complex_words):\n",
    "    return 0.4 * (avg_sentence_length + percentage_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.863157894736842\n"
     ]
    }
   ],
   "source": [
    "fog_index = calculate_fog_index(avg_sentence_length, percentage_complex_words)\n",
    "print(fog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_words_per_sentence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words) / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5\n"
     ]
    }
   ],
   "source": [
    "avg_words_per_sentence = calculate_avg_words_per_sentence(sample_text)\n",
    "print(avg_words_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_complex_words(tokens):\n",
    "    return sum(1 for word in tokens if sum(1 for char in word if char in 'aeiou') > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "complex_word_count = count_complex_words(sample_tokens)\n",
    "print(complex_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    syllables = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        syllables += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            syllables += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        syllables -= 1\n",
    "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
    "        syllables += 1\n",
    "    if syllables == 0:\n",
    "        syllables += 1\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "sample_word = \"complex\"\n",
    "syllable_count = count_syllables(sample_word)\n",
    "print(syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_syllables_per_word(tokens):\n",
    "    return np.mean([count_syllables(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4210526315789473\n"
     ]
    }
   ],
   "source": [
    "syllables_per_word = calculate_syllables_per_word(sample_tokens)\n",
    "print(syllables_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "personal_pronouns_count = count_personal_pronouns(sample_text)\n",
    "print(personal_pronouns_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_length(tokens):\n",
    "    return np.mean([len(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.105263157894737\n"
     ]
    }
   ],
   "source": [
    "avg_word_length = calculate_avg_word_length(sample_tokens)\n",
    "print(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = load_stop_words('StopWords')\n",
    "positive_words = load_dictionary('MasterDictionary/positive-words.txt')\n",
    "negative_words = load_dictionary('MasterDictionary/negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(title, text):\n",
    "    cleaned_tokens = clean_text(text)\n",
    "    total_words = len(cleaned_tokens)\n",
    "    positive_score = calculate_positive_score(cleaned_tokens)\n",
    "    negative_score = calculate_negative_score(cleaned_tokens)\n",
    "    polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "    subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)\n",
    "    avg_sentence_length = calculate_avg_sentence_length(text)\n",
    "    percentage_complex_words = calculate_percentage_complex_words(cleaned_tokens)\n",
    "    fog_index = calculate_fog_index(avg_sentence_length, percentage_complex_words)\n",
    "    avg_words_per_sentence = calculate_avg_words_per_sentence(text)\n",
    "    complex_word_count = count_complex_words(cleaned_tokens)\n",
    "    syllables_per_word = calculate_syllables_per_word(cleaned_tokens)\n",
    "    personal_pronouns = count_personal_pronouns(text)\n",
    "    avg_word_length = calculate_avg_word_length(cleaned_tokens)\n",
    "\n",
    "    return {\n",
    "        \"POSITIVE SCORE\": positive_score,\n",
    "        \"NEGATIVE SCORE\": negative_score,\n",
    "        \"POLARITY SCORE\": polarity_score,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "        \"WORD COUNT\": total_words,\n",
    "        \"SYLLABLE PER WORD\": syllables_per_word,\n",
    "        \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "        \"AVG WORD LENGTH\": avg_word_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles_from_dir(dir_path):\n",
    "    articles = {}\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 2:\n",
    "                    title = lines[0].strip()\n",
    "                    text = ' '.join(lines[1:]).strip()\n",
    "                    articles[filename.replace('.txt', '')] = (title, text)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_results(input_excel, output_excel, articles_dir):\n",
    "    input_df = pd.read_excel(input_excel)\n",
    "\n",
    "    articles = extract_articles_from_dir(articles_dir)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for index, row in input_df.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        if url_id in articles:\n",
    "            title, text = articles[url_id]\n",
    "            analysis_result = analyze_text(title, text)\n",
    "            analysis_result['URL_ID'] = url_id\n",
    "            results.append(analysis_result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    output_df = input_df.merge(results_df, on='URL_ID', how='left')\n",
    "\n",
    "    for col in results_df.columns:\n",
    "        if col in output_df.columns:\n",
    "            output_df[col] = results_df[col]\n",
    "\n",
    "    columns_order = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "                    'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "                    'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
    "                    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
    "    \n",
    "    output_df = output_df[columns_order]\n",
    "    output_df.to_excel(output_excel, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_excel = 'Input.xlsx'\n",
    "output_excel = 'Output Data Structure.xlsx'\n",
    "articles_dir = 'extracted_articles'\n",
    "\n",
    "process_and_save_results(input_excel, output_excel, articles_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.071028</td>\n",
       "      <td>22.186441</td>\n",
       "      <td>0.478505</td>\n",
       "      <td>9.065978</td>\n",
       "      <td>22.186441</td>\n",
       "      <td>256</td>\n",
       "      <td>535</td>\n",
       "      <td>2.244860</td>\n",
       "      <td>12</td>\n",
       "      <td>6.968224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>57</td>\n",
       "      <td>27</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.107417</td>\n",
       "      <td>26.387097</td>\n",
       "      <td>0.609974</td>\n",
       "      <td>10.798828</td>\n",
       "      <td>26.387097</td>\n",
       "      <td>477</td>\n",
       "      <td>782</td>\n",
       "      <td>2.566496</td>\n",
       "      <td>6</td>\n",
       "      <td>7.636829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.100977</td>\n",
       "      <td>25.847826</td>\n",
       "      <td>0.661238</td>\n",
       "      <td>10.603626</td>\n",
       "      <td>25.847826</td>\n",
       "      <td>406</td>\n",
       "      <td>614</td>\n",
       "      <td>2.872964</td>\n",
       "      <td>13</td>\n",
       "      <td>8.433225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              33               5        0.736842            0.071028   \n",
       "1              57              27        0.357143            0.107417   \n",
       "2              38              24        0.225806            0.100977   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0            22.186441                     0.478505   9.065978   \n",
       "1            26.387097                     0.609974  10.798828   \n",
       "2            25.847826                     0.661238  10.603626   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                         22.186441                 256         535   \n",
       "1                         26.387097                 477         782   \n",
       "2                         25.847826                 406         614   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0           2.244860                 12         6.968224  \n",
       "1           2.566496                  6         7.636829  \n",
       "2           2.872964                 13         8.433225  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(output_excel)\n",
    "df.head()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
